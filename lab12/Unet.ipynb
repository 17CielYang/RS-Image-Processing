{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet深度学习图像分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data.dataset as Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义数据集类\n",
    "class _dataset(Dataset.Dataset):\n",
    "    def __init__(self, csv_dir):\n",
    "        self.csv_dir = csv_dir          \n",
    "        self.names_list = []\n",
    "        self.size = 0\n",
    "        self.transform = transforms.ToTensor()\n",
    "        #读入csv文件\n",
    "        if not os.path.isfile(self.csv_dir):\n",
    "            print(self.csv_dir + ':txt file does not exist!')\n",
    "        file = open(self.csv_dir)\n",
    "        for f in file:\n",
    "            self.names_list.append(f)\n",
    "            self.size += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #读取图像路径并打开图像\n",
    "        image_path = self.names_list[idx].split(',')[0]\n",
    "        image = Image.open(image_path)\n",
    "        #读取标签路径并打开标签图像\n",
    "        label_path = self.names_list[idx].split(',')[1].strip('\\n')\n",
    "        label = Image.open(label_path)\n",
    "        #转为tensor形式\n",
    "        image = self.transform(image)\n",
    "        label = torch.from_numpy(np.array(label))\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义unet模型相关类\n",
    "class conv_conv(nn.Module):\n",
    "    ''' conv_conv: (conv[3*3] + BN + ReLU) *2 '''\n",
    "    def __init__(self, in_channels, out_channels, bn_momentum=0.1):\n",
    "        super(conv_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(out_channels, momentum=bn_momentum),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(out_channels, momentum=bn_momentum),\n",
    "            nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.conv(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class downconv(nn.Module):\n",
    "    ''' downconv: conv_conv => maxpool[2*2] '''\n",
    "    def __init__(self, in_channels, out_channels, bn_momentum=0.1):\n",
    "        super(downconv, self).__init__()\n",
    "        self.conv = conv_conv(in_channels, out_channels, bn_momentum)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.conv(X)\n",
    "        pool_X = self.pool(X)\n",
    "        return pool_X, X\n",
    "        \n",
    "class upconv_concat(nn.Module):\n",
    "    ''' upconv_concat: upconv[2*2] => cat => conv_conv '''\n",
    "    def __init__(self, in_channels, out_channels, bn_momentum=0.1):\n",
    "        super(upconv_concat, self).__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = conv_conv(in_channels, out_channels, bn_momentum)\n",
    "\n",
    "    def forward(self, X1, X2):\n",
    "        X1 = self.upconv(X1)\n",
    "        feature_map = torch.cat((X2, X1), dim=1)\n",
    "        X1 = self.conv(feature_map)\n",
    "        return X1\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    ''' UNet(3-level): downconv *3 => conv_conv => upconv *3 => conv[1*1]'''\n",
    "    def __init__(self, in_channels, out_channels, starting_filters=32, bn_momentum=0.1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.conv1 = downconv(in_channels, starting_filters, bn_momentum) \n",
    "        self.conv2 = downconv(starting_filters, starting_filters * 2, bn_momentum)\n",
    "        self.conv3 = downconv(starting_filters * 2, starting_filters * 4, bn_momentum)\n",
    "        self.conv4 = conv_conv(starting_filters * 4, starting_filters * 8, bn_momentum)\n",
    "        self.upconv1 = upconv_concat(starting_filters * 8, starting_filters * 4, bn_momentum)\n",
    "        self.upconv2 = upconv_concat(starting_filters * 4, starting_filters * 2, bn_momentum)\n",
    "        self.upconv3 = upconv_concat(starting_filters * 2, starting_filters, bn_momentum)\n",
    "        self.conv_out = nn.Conv2d(starting_filters, out_channels, kernel_size=1, padding=0, stride=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X, conv1 = self.conv1(X)\n",
    "        X, conv2 = self.conv2(X)\n",
    "        X, conv3 = self.conv3(X)\n",
    "        X = self.conv4(X)\n",
    "        X = self.upconv1(X, conv3)\n",
    "        X = self.upconv2(X, conv2)\n",
    "        X = self.upconv3(X, conv1)\n",
    "        X = self.conv_out(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1088\n",
      "EPOCH 1/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [08:19<00:00,  7.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.675912\n",
      "EPOCH 2/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [08:58<00:00,  7.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.686601\n",
      "EPOCH 3/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [08:51<00:00,  7.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.539788\n",
      "EPOCH 4/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [08:39<00:00,  7.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.759555\n",
      "EPOCH 5/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [27:50<00:00, 24.56s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.390251\n",
      "EPOCH 6/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [08:16<00:00,  7.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.545886\n",
      "EPOCH 7/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [08:47<00:00,  7.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.427047\n",
      "EPOCH 8/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [53:42<00:00, 47.39s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.359762\n",
      "EPOCH 9/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [22:17<00:00, 19.67s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.376757\n",
      "EPOCH 10/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [08:52<00:00,  7.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.556619\n",
      "EPOCH 11/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [09:00<00:00,  7.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.372887\n",
      "EPOCH 12/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [08:51<00:00,  7.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.461330\n",
      "EPOCH 13/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [08:17<00:00,  7.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.391213\n",
      "EPOCH 14/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [08:21<00:00,  7.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.372106\n",
      "EPOCH 15/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [08:30<00:00,  7.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.360246\n",
      "EPOCH 16/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [08:32<00:00,  7.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.379426\n",
      "EPOCH 17/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [09:54<00:00,  8.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.481932\n",
      "EPOCH 18/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [08:48<00:00,  7.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.351724\n",
      "EPOCH 19/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [08:22<00:00,  7.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.316849\n",
      "EPOCH 20/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [08:39<00:00,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.348284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 以下为训练部分\n",
    "########################################################################\n",
    "# 载入数据\n",
    "datatrain = _dataset('dataset/train.csv')\n",
    "print(\"train:\",len(datatrain))\n",
    "train_dataloader = DataLoader(datatrain, batch_size=16, shuffle=True)\n",
    "# 实例化模型\n",
    "unet = UNet(in_channels=3,out_channels=6)\n",
    "# 设置损失函数为交叉熵函数\n",
    "Loss_function = nn.CrossEntropyLoss()\n",
    "# 设置优化方法为自适应动量法\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=0.001)\n",
    "# epoch代表遍历完所有样本的过程，将epoch设置10，即遍历完样本10次\n",
    "epoch_num = 20\n",
    "for epoch in range(epoch_num):\n",
    "    print('EPOCH %d/%d'%(epoch + 1,epoch_num))\n",
    "    print('-'*10)\n",
    "    #批量输入数据\n",
    "    for images, labels in tqdm(train_dataloader):\n",
    "        outputs = unet(images)                          # 将影像输入网络得到输出 \n",
    "        optimizer.zero_grad()                           # 将梯度置为0\n",
    "        loss = Loss_function(outputs, labels.long())    # 计算损失\n",
    "        loss.backward()                                 # 损失反向传播\n",
    "        optimizer.step()                                # 优化更新网络权重   \n",
    "    # 打印损失值\n",
    "    print('loss is %f'%(loss.item()))\n",
    "    # 保存模型\n",
    "    torch.save(unet,'dataset/model_epoch%d.pkl'%(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j2/jw1hqrnd7p7b025g29bgb2s00000gn/T/ipykernel_82170/99018616.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net = torch.load('dataset/model_epoch20.pkl', map_location=lambda storage, loc: storage)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:42<00:00,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy:  0.7878938001744887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 以下为验证部分\n",
    "#########################################################################\n",
    "# 载入数据\n",
    "datatest = _dataset('dataset/test.csv')\n",
    "print(\"test:\",len(datatest))\n",
    "test_dataloader = DataLoader(datatest, batch_size=16, shuffle=False)\n",
    "# 载入训练好的模型\n",
    "net = torch.load('dataset/model_epoch20.pkl', map_location=lambda storage, loc: storage)\n",
    "# 设置为测试模式\n",
    "net.eval()\n",
    "running_correct_full = 0\n",
    "img_no = 0\n",
    "for images, labels in tqdm(test_dataloader):\n",
    "    outputs = net(images)                                   # 得到网络输出\n",
    "    _, preds = torch.max(outputs, 1)                        # 预测类别值\n",
    "    running_correct_full+=torch.sum(preds == labels.data)   # 统计分类正确像元数\n",
    "    # 输出结果查看\n",
    "    images = images.numpy()\n",
    "    preds = preds.numpy()\n",
    "    labels = labels.numpy()\n",
    "    for j in range(labels.shape[0]):\n",
    "        img = transforms.ToPILImage()(torch.from_numpy(images[j]))\n",
    "        label = Image.fromarray(labels[j].astype(np.uint8))\n",
    "        predect = transforms.ToPILImage()(preds[j].astype(np.uint8))\n",
    "        image_fn = 'dataset/save/img' + str(img_no) + '.png'\n",
    "        label_fn = 'dataset/save/lab' + str(img_no) + '.png'\n",
    "        predect_fn = 'dataset/save/pre' + str(img_no) + '.png'\n",
    "        img.save(image_fn)\n",
    "        label.save(label_fn)\n",
    "        predect.save(predect_fn)\n",
    "        img_no = img_no+1\n",
    "# 计算测试精度\n",
    "acc = running_correct_full.double()/(len(datatest)*labels.shape[1]*labels.shape[2])\n",
    "print(\"Overall Accuracy: \",acc.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
